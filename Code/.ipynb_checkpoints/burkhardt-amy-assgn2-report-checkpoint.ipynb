{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: Implementing the Viterbi Matrix: A Hidden Markov Model Approach for Predicing Part-of-Speech Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Unknown words and smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load burkhardt-amy-assgn2.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tags = ['CC', 'CD',\n",
    "        'DT',\n",
    "        'EX',\n",
    "        'FW',\n",
    "        'IN',\n",
    "        'JJ', 'JJR', 'JJS',\n",
    "        'LS',\n",
    "        'MD',\n",
    "        'NN', 'NNS', 'NNP', 'NNPS',\n",
    "        'PDT', 'POS', 'PRP', 'PRP$',\n",
    "        'RB', 'RBR', 'RBS', 'RP',\n",
    "        'SYM',\n",
    "        'TO',\n",
    "        'UH',\n",
    "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "        'WDT', 'WP', 'WP$', 'WRB',\n",
    "        '$', '#', '\"', '(', ')', ',', '.', ':'\n",
    "        ]\n",
    "\n",
    "\n",
    "def ngram_dict(data, ngrams = \"tag_word\"):\n",
    "    \"\"\"\n",
    "    Creates dict of ngrams (key) and count (value).\n",
    "\n",
    "    Arguments:\n",
    "        data: DataFrame with 'tag' and 'word' colum\n",
    "        negrams: denote type of ngram (unigram or bigram) and if want words or tags: word_word or tag_word\n",
    "    Returns:\n",
    "        A dict where key is either a unigram or a bigram tuple, and value is the count of the ngrams\n",
    "    \"\"\"\n",
    "    if ngrams == \"tag_tag\":\n",
    "        col_1 = data['tag']\n",
    "        col_2 = col_1[1:col_1.shape[0]]\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "        ngram_count[('', col_1[0])] += 1\n",
    "\n",
    "    if ngrams == \"tag_word\":  # not really bi-grams, just getting count of tag,word\n",
    "        col_1 = data['word']\n",
    "        col_2 = data['tag']\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "\n",
    "    if ngrams == 'tag':\n",
    "        ngram_count = dict(Counter(data.tag))\n",
    "\n",
    "    if ngrams == 'word':\n",
    "        ngram_count = dict(Counter(data.word))\n",
    "\n",
    "    return ngram_count\n",
    "\n",
    "\n",
    "def fixed_vocabulary(df, min_freq=2):\n",
    "    \"\"\"\n",
    "    Provides list of fixed vocabulary for the observation likelihood matrix\n",
    "\n",
    "    Arguments:\n",
    "        train_data: data used for training the probability matrices\n",
    "        min_freq: value where if the frequency of the word in training data is less than, then the word is concered UNK\n",
    "\n",
    "    Returns:\n",
    "        vocabulary: list of vocabulary that is used as the columns of the observation matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    unigrams = ngram_dict(df, \"word\")\n",
    "    unknowns = {key: value for key, value in unigrams.items() if value < min_freq}\n",
    "    unknowns = unknowns.fromkeys(unknowns, 'UNK')\n",
    "    df['word'] = df['word'].replace(unknowns)\n",
    "    vocab = ngram_dict(df, \"word\")\n",
    "    vocabulary = list(vocab.keys())\n",
    "    vocabulary.remove('')\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def compute_transition_matrix (tags, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the transition matrix (len(tags)+1 x len(tags))\n",
    "\n",
    "    Arguments:\n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        bigram_counts: count of bigrams of POS tags in training data (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "\n",
    "    Returns: 45 x 44 matrix of transition probabilities for all possible POS tags\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transition = [] # list of transition probabilities\n",
    "\n",
    "    # first compute the starting probabilities\n",
    "\n",
    "    for x in tags:\n",
    "            pair = ('',x) # here the period denotes the start of a sentence. Not very confident about this\n",
    "            denominator = unigram_counts[''] + len(tags)\n",
    "            try:\n",
    "                 numerator = bigram_counts[pair] + 1\n",
    "            except:\n",
    "                 numerator = 1\n",
    "            transition.append(numerator / denominator)\n",
    "\n",
    "\n",
    "    # then compute everything else\n",
    "\n",
    "    for x in tags:\n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try:\n",
    "                denominator = unigram_counts[x] + len(tags)\n",
    "            except:\n",
    "                denominator = len(tags)\n",
    "            try:\n",
    "                numerator = bigram_counts[pair] + 1\n",
    "            except:\n",
    "                numerator = 1\n",
    "            transition.append(numerator / denominator)\n",
    "\n",
    "    transition = np.array(transition)\n",
    "    tran_matrix = transition.reshape(len(tags)+1, len(tags))\n",
    "    return tran_matrix\n",
    "\n",
    "\n",
    "def compute_observation_matrix (tags, vocabulary, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the observation matrix (tags, vocabulary)\n",
    "\n",
    "    Arguments:\n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        vocabulary: words that appear in the training set. Any words that appear less than 2 times = UNK\n",
    "        bigram_counts: count of bigrams of (tag, word) (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "\n",
    "    Returns: len(tags) x len(vocabulary) matrix of transition probabilities for all possible POS tags\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    observations = [] # list of observation likelihoods\n",
    "    for x in tags:\n",
    "        for y in vocabulary:\n",
    "            pair = (y, x)\n",
    "            try:\n",
    "                denominator = unigram_counts[x] + len(vocabulary)\n",
    "            except:\n",
    "                 denominator = len(vocabulary)\n",
    "            try:\n",
    "                 numerator = bigram_counts[pair] + 1\n",
    "            except: numerator = 1\n",
    "            observations.append(numerator / denominator)\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    obs_matrix = observations.reshape(len(tags),len(vocabulary))\n",
    "    return obs_matrix\n",
    "\n",
    "\n",
    "def viterbi (transition, observations, events):\n",
    "    \"\"\" Computes sequnce of hidden states, given observed events.\n",
    "    Arguments:\n",
    "        transition: transition matrix with start probabilites as first row\n",
    "        observations: observation liklihood matrix, with states as rows, and vocabulary as columns\n",
    "        events: sequence of observed events\n",
    "\n",
    "    Returns:\n",
    "        generator, which yields the states\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = transition.shape[1]\n",
    "    n_events = len(events)\n",
    "    v = np.zeros((n_states, n_events))\n",
    "    bp = v.copy()\n",
    "\n",
    "    # initialization step\n",
    "    for s in range(n_states):\n",
    "        v[s,0] = transition[0,s] * observations[s, events[0]]\n",
    "\n",
    "    # induction step\n",
    "    for t in range (1, n_events):\n",
    "        for s in range(n_states):\n",
    "            tmp = []\n",
    "            for s_prime in range (n_states):\n",
    "                prev_t = v[s_prime, t-1]\n",
    "                tran_s_prime_to_s = transition[s_prime + 1, s]\n",
    "                obser_s_given_t = observations[s, events[t]]\n",
    "                tmp.append(prev_t * tran_s_prime_to_s * obser_s_given_t) # still need to changet thos to adding logs\n",
    "            # now that all interim probabilities have been computed for given state, get max\n",
    "            # and also store the index of the argmax\n",
    "            v[s,t] = max(tmp) # log will be negative; so insetad of\n",
    "            bp[s,t] = np.argmax(tmp) # take argmin\n",
    "\n",
    "    # termination step\n",
    "    q = np.argmax(v[:, n_events-1]) # want to get the argmax of the final time -- it will return a state index\n",
    "\n",
    "    # back reference step\n",
    "    for i in reversed(range(n_events)):\n",
    "        yield q\n",
    "        q = int(bp[q,i])\n",
    "\n",
    "\n",
    "def get_sequence(viterbi_gen, names_events):\n",
    "    \"\"\" translate viterbi generater into a sequence of state names\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for state in viterbi_gen:\n",
    "        name = names_events[state]\n",
    "        sequence.insert(0, name)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    df = pd.read_table(\"{}\".format(path), '\\t',\n",
    "                          header=None,\n",
    "                          skip_blank_lines=False,\n",
    "                          keep_default_na=False,\n",
    "                          names=['word_Num', 'word', 'tag'])\n",
    "    return df\n",
    "\n",
    "def get_vocabulary(df, min_freq=2):\n",
    "    \"\"\"\n",
    "    Provides list of fixed vocabulary for the observation likelihood matrix\n",
    "\n",
    "    Arguments:\n",
    "        train_data: data used for training the probability matrices\n",
    "        min_freq: value where if the frequency of the word in training data is less than, then the word is concered UNK\n",
    "\n",
    "    Returns:\n",
    "        vocabulary: list of vocabulary that is used as the columns of the observation matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    unigrams = ngram_dict(df, \"word\")\n",
    "    unknowns = {key: value for key, value in unigrams.items() if value < min_freq}\n",
    "    unknowns = unknowns.fromkeys(unknowns, 'UNK')\n",
    "    df['word'] = df['word'].replace(unknowns)\n",
    "    vocab = ngram_dict(df, \"word\")\n",
    "    vocabulary = list(vocab.keys())\n",
    "    vocabulary.remove('')\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def get_transition (df):\n",
    "    bigram_tag_counts = ngram_dict(df, \"tag_tag\")\n",
    "    unigram_tag_counts = ngram_dict(df, \"tag\")\n",
    "    transitions = compute_transition_matrix(tags, bigram_tag_counts, unigram_tag_counts)\n",
    "    print(\"confirm that rows of transition table sum closely to 1\")\n",
    "    print(np.sum(transitions, axis=1))\n",
    "    return transitions\n",
    "\n",
    "\n",
    "def get_observation (df, vocabulary):\n",
    "\n",
    "    bigram_counts = ngram_dict(df, \"tag_word\")\n",
    "    unigram_counts = ngram_dict(df, \"tag\")\n",
    "    observations = compute_observation_matrix(tags, vocabulary, bigram_counts, unigram_counts)\n",
    "    print(\"confirm that the rows of observation likelihood table sum closely to 1\")\n",
    "    print(np.sum(observations, axis =1))\n",
    "    return observations\n",
    "\n",
    "\n",
    "def read_in_print_out(df, transitions, observations, vocabulary, path):\n",
    "    \"\"\"\n",
    "    Predicts the pos tags in a file that can be recognized to compare against the true labels.\n",
    "\n",
    "    Arguments:\n",
    "        df: either the training or validation dataset\n",
    "        transitions: name of the transition probability matrix\n",
    "        observations: name of the observation matrix\n",
    "        path: file location of the predicted pos file\n",
    "        df: full_train, partial train, test or something else\n",
    "\n",
    "    returns:\n",
    "        tab-delimited file of predicted scores\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = df['word'].tolist()\n",
    "\n",
    "    def sent(seq, sep):\n",
    "        g = []\n",
    "        for el in seq:\n",
    "            if el == sep:\n",
    "                yield g\n",
    "                g = []\n",
    "            g.append(el)\n",
    "        yield g\n",
    "    g = []\n",
    "\n",
    "    result = list(sent(sentences, ''))\n",
    "\n",
    "    def get_events(new_sent, vocabulary):\n",
    "        events = []\n",
    "        for word in new_sent:\n",
    "            try:\n",
    "                events.append(vocabulary.index(word))\n",
    "            except:\n",
    "                events.append(vocabulary.index('UNK'))\n",
    "        return events\n",
    "\n",
    "    all_pos = []\n",
    "    counter = 0\n",
    "    for new_sent in result:\n",
    "        if counter > 0:\n",
    "            new_sent.pop(0)\n",
    "        tagger = viterbi(transitions, observations, get_events(new_sent))\n",
    "        print(new_sent)\n",
    "        sequence = get_sequence(tagger, tags)\n",
    "        sequence.insert(len(sequence), '') #add space at the end\n",
    "        all_pos.append(sequence)\n",
    "        counter += 1\n",
    "\n",
    "        flat_list = [item for sublist in all_pos for item in sublist]\n",
    "\n",
    "        df = pd.DataFrame({'col': flat_list})\n",
    "\n",
    "    df = pd.DataFrame({'col':flat_list})\n",
    "    df['tag'] = df\n",
    "\n",
    "    predicted = df.to_csv(\"{}burkhardt-amy-assgn2-{}-output.txt\".format(path,df), sep='\\t', index=False, header=False)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirm that rows of transition table sum closely to 1\n",
      "[ 1.00007896  1.          1.          0.9923554   1.          0.94901961\n",
      "  1.          0.9950701   1.          1.          0.84615385  1.\n",
      "  0.97792019  0.99894715  0.97271268  1.          1.          1.\n",
      "  0.99980607  1.          0.98761545  1.          1.          1.          1.\n",
      "  1.          1.          0.99947871  1.          0.98099762  0.99438202\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          0.00347387\n",
      "  1.        ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_events() missing 1 required positional argument: 'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d1e54666840c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mread_in_print_out\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-ca61fe3eb2d5>\u001b[0m in \u001b[0;36mread_in_print_out\u001b[0;34m(df, transitions, observations, path)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mnew_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_events() missing 1 required positional argument: 'vocabulary'"
     ]
    }
   ],
   "source": [
    "# run all training data through\n",
    "train = get_data(\"Data/rand_training.txt\") # either random sample or all data\n",
    "vocabulary = get_vocabulary(train, 2)\n",
    "transition = get_transition(train)\n",
    "observation = get_observation(train, vocabulary)\n",
    "read_in_print_out (train, transition, observation, \"Data/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
