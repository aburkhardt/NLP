{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB classifier\n",
    "\n",
    "\n",
    "We use scikit-learn's implementation of SVM and its cross validation tools. http://scikit-learn.org/\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install all of the python dependencies for this notbook in a virtual environment:\n",
    "\n",
    "```bash\n",
    "# create environment in directory named 'venv'\n",
    "python -m venv venv\n",
    "# or:\n",
    "# virtualenv venv\n",
    "\n",
    "# activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# install dependencies\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "iteration=\"not-critical\"\n",
    "model_filename = \"/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/saved_models/best_svc_{}.pickle\".format(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data sets\n",
    "\n",
    "Here we parse data from our training files, and then randomly select a portion to be held out for evaluation. The training set is used to both train the SVM classifier and select parameters using k-fold cross validation.\n",
    "\n",
    "The `parse_training_data()` function is provided in the external `class_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/training_data\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/training_data/\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data from files\n",
    "classes = ['NEG', 'POS']\n",
    "docs, targets = parse_training_data(['NEG.txt'.format(iteration), 'POS-{}.txt'.format(iteration)], classes)\n",
    "\n",
    "# convert the targets array of strings to binary labels (0=NEG, 1=POS)\n",
    "lb = LabelBinarizer(sparse_output=False)\n",
    "lb.fit(classes)\n",
    "bin_targets = lb.transform(targets).ravel()\n",
    "\n",
    "# split data set into to training and evaluation sets\n",
    "# X_test/y_test are held out and not used during the\n",
    "# k-fold training and parameter search below\n",
    "#\n",
    "# The percentage of samples to hod out is determined by the `test_size`\n",
    "# parameter\n",
    "# for this iter2, the holdout is only going to be 10% \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs, bin_targets, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sklearn pipeline\n",
    "\n",
    "Here we setup a scikit-learn pipeline to create vectors from our training sample vocabulary (`CountVectorizer`), normalize words based on frequency (`TfidfTransformer`), and train a SVM classifier (`SVC`). http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "We evaluate parameters based on th `fscore_prec` which is a weighted fscore which favors precision (beta < 1). We also calculate accuracy, precision, recall, and f1 scores for each of the k-fold training sessions.\n",
    "\n",
    "Using a pipeline makes it easy to search a range of hyperparameters using sklearn's `GridSearchCV`. http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    'clf__alpha': np.linspace(0.05, 0.2, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=.5)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05      , 0.06666667, 0.08333333, 0.1       , 0.11666667,\n",
       "       0.13333333, 0.15      , 0.16666667, 0.18333333, 0.2       ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linspace(0.05, 0.2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1200 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 34.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 39.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x1122fe730>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x1152258c8>], 'vect__stop_words': ['english', None]...([0.05   , 0.06667, 0.08333, 0.1    , 0.11667, 0.13333, 0.15   ,\n",
       "       0.16667, 0.18333, 0.2    ])},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=0.5)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.06666666666666668,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__max_df': 0.45555555555555555,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.860\n",
      "recall: 0.522\n",
      "f1: 0.646\n",
      "fscore_prec: 0.758\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.78      0.96      0.86        47\n",
      "        POS       0.80      0.38      0.52        21\n",
      "\n",
      "avg / total       0.78      0.78      0.75        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.38095238095238093\n",
      "0.5161290322580645\n",
      "0.7794117647058824\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We check how it works by running the best classifier from the grid search on our held out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    'clf__alpha': np.linspace(0.05, 0.2, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=1)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1200 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 37.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 41.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x1122fe730>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x1152258c8>], 'vect__stop_words': ['english', None]...([0.05   , 0.06667, 0.08333, 0.1    , 0.11667, 0.13333, 0.15   ,\n",
       "       0.16667, 0.18333, 0.2    ])},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=1)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.05,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__max_df': 0.45555555555555555,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.811\n",
      "recall: 0.549\n",
      "f1: 0.654\n",
      "fscore_prec: 0.654\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.80      0.91      0.85        47\n",
      "        POS       0.71      0.48      0.57        21\n",
      "\n",
      "avg / total       0.77      0.78      0.76        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n",
      "0.47619047619047616\n",
      "0.5714285714285714\n",
      "0.7794117647058824\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    'clf__alpha': np.linspace(0.05, 0.2, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=1.5)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1200 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 27.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 38.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x1122fe730>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x1152258c8>], 'vect__stop_words': ['english', None]...([0.05   , 0.06667, 0.08333, 0.1    , 0.11667, 0.13333, 0.15   ,\n",
       "       0.16667, 0.18333, 0.2    ])},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=1.5)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.05,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.6111111111111112,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.773\n",
      "recall: 0.565\n",
      "f1: 0.649\n",
      "fscore_prec: 0.613\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.82      0.89      0.86        47\n",
      "        POS       0.71      0.57      0.63        21\n",
      "\n",
      "avg / total       0.79      0.79      0.79        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "0.5714285714285714\n",
      "0.6315789473684211\n",
      "0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
