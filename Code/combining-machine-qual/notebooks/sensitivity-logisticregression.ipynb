{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR classifier\n",
    "\n",
    "\n",
    "We use scikit-learn's implementation of SVM and its cross validation tools. http://scikit-learn.org/\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install all of the python dependencies for this notbook in a virtual environment:\n",
    "\n",
    "```bash\n",
    "# create environment in directory named 'venv'\n",
    "python -m venv venv\n",
    "# or:\n",
    "# virtualenv venv\n",
    "\n",
    "# activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# install dependencies\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "iteration=\"not-critical\"\n",
    "model_filename = \"/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/saved_models/best_svc_{}.pickle\".format(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data sets\n",
    "\n",
    "Here we parse data from our training files, and then randomly select a portion to be held out for evaluation. The training set is used to both train the SVM classifier and select parameters using k-fold cross validation.\n",
    "\n",
    "The `parse_training_data()` function is provided in the external `class_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/training_data\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/amyburkhardt/Documents/NLP/Code/combining-machine-qual/training_data/\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data from files\n",
    "classes = ['NEG', 'POS']\n",
    "docs, targets = parse_training_data(['NEG.txt'.format(iteration), 'POS-{}.txt'.format(iteration)], classes)\n",
    "\n",
    "# convert the targets array of strings to binary labels (0=NEG, 1=POS)\n",
    "lb = LabelBinarizer(sparse_output=False)\n",
    "lb.fit(classes)\n",
    "bin_targets = lb.transform(targets).ravel()\n",
    "\n",
    "# split data set into to training and evaluation sets\n",
    "# X_test/y_test are held out and not used during the\n",
    "# k-fold training and parameter search below\n",
    "#\n",
    "# The percentage of samples to hod out is determined by the `test_size`\n",
    "# parameter\n",
    "# for this iter2, the holdout is only going to be 10% \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs, bin_targets, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sklearn pipeline\n",
    "\n",
    "Here we setup a scikit-learn pipeline to create vectors from our training sample vocabulary (`CountVectorizer`), normalize words based on frequency (`TfidfTransformer`), and train a SVM classifier (`SVC`). http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "We evaluate parameters based on th `fscore_prec` which is a weighted fscore which favors precision (beta < 1). We also calculate accuracy, precision, recall, and f1 scores for each of the k-fold training sessions.\n",
    "\n",
    "Using a pipeline makes it easy to search a range of hyperparameters using sklearn's `GridSearchCV`. http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=.5)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x10c1801e0>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x10da6f048>], 'vect__stop_words': ['english', None], 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tfidf__use_idf': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=0.5)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.45555555555555555,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.682\n",
      "recall: 0.631\n",
      "f1: 0.655\n",
      "fscore_prec: 0.671\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.76      0.79      0.77        47\n",
      "        POS       0.47      0.43      0.45        21\n",
      "\n",
      "avg / total       0.67      0.68      0.67        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47368421052631576\n",
      "0.42857142857142855\n",
      "0.45\n",
      "0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEl1JREFUeJzt3X+QXmdd9/H3h4RSkJKCSx2mCaSYoGYqUFhLGWawDNhpO5qqg9A8MliePkSRCvgDxdGBWsdRQcRfVYwWwY5SWmbE6FOtj30q8XEIT9IpLbTYuoYfXYtTCjWOtqQUvv5xnzTrZnPtyd2cve/dvl8zOzk/rnPu716zu5+c69znulNVSJJ0LI+bdAGSpOlmUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUtH7SBRyvmZmZ2rx586TLkKRV5eabb76vqp4+zrGrLig2b97M/v37J12GJK0qST437rEOPUmSmgwKSVKTQSFJajIoJElNBoUkqWmwoEjyviT3JvnUMfYnyW8lmUtyW5IXDFWLJGl8Q15RvB84v7H/AmBr97UT+L0Ba5EkjWmwoKiqPcCXG00uAv64RvYCpyZ5xlD1SJLGM8l7FKcDdy9Yn++2NT300GD1SJKWMMkns7PEtlqyYbKT0fAUMzPPHrImSdIik7yimAc2LVjfCNyzVMOq2lVVs1U1u2HDU1ekOEnSyCSDYjfw2u7dT+cAB6vqCxOsR5K0hMGGnpJ8EDgXmEkyD7wDeDxAVb0XuB64EJgDHgBeN1QtkqTxDRYUVbVjmf0FvHGo15cknRg+mS1JajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklS06BBkeT8JHcmmUvytiX2PzPJTUluSXJbkguHrEeSdPwGC4ok64ArgQuAbcCOJNsWNft54NqqOgu4GPjdoeqRJI1nyCuKs4G5qjpQVQ8B1wAXLWpTwFO65Q3APQPWI0kaw/oBz306cPeC9XngRYvaXA78TZIfA74BeMWA9UiSxjDkFUWW2FaL1ncA76+qjcCFwNVJjqopyc4k+5PsP3jw/gFKlSQdy5BBMQ9sWrC+kaOHli4FrgWoqo8BJwMzi09UVbuqaraqZjdseOpA5UqSljJkUOwDtiY5I8lJjG5W717U5vPAywGSfBujoPjigDVJko7TYEFRVQ8DlwE3AJ9m9O6m25NckWR71+wngdcnuRX4IHBJVS0enpIkTVBW29/lLVtma25u/6TLkKRVJcnNVTU7zrE+mS1JajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklS0/q+DZOcDjxr4TFVtWeIoiRJ06NXUCT5VeDVwB3A17rNBTSDIsn5wG8C64A/rKpfWaLNq4DLu/PdWlX/o2/xkqTh9b2i+F7gW6rqUN8TJ1kHXAl8FzAP7Euyu6ruWNBmK/CzwEuq6v4kp/UvXZK0EvreozgAPP44z302MFdVB6rqIeAa4KJFbV4PXFlV9wNU1b3H+RqSpIH1vaJ4APhEkhuBR64qqupNjWNOB+5esD4PvGhRm+cAJPkHRsNTl1fVX/esSZK0AvoGxe7u63hkiW21xOtvBc4FNgJ/n+TMqvq3/3aiZCewE2Bm5tnHWYYk6dHoFRRV9YEkJ9FdAQB3VtVXlzlsHti0YH0jcM8SbfZ25/pMkjsZBce+Ra+/C9gFsGXL7OKwkSQNqNc9iiTnAv/E6Ob07wJ3JXnpMoftA7YmOaMLmYs5+qrkI8DLuteYYRREB3pXL0kaXN+hp3cD51XVnQBJngN8EHjhsQ6oqoeTXAbcwOj+w/uq6vYkVwD7q2p3t++8JIffdvvWqvrS+N+OJOlES9XyIzlJbquq5y63bSVs2TJbc3P7V/plJWlVS3JzVc2Oc2zfK4r9Sa4Cru7WfxC4eZwXlCStLn2D4g3AG4E3MXo30x5G9yokSWtc33c9HQJ+vfuSJD2GNIMiybVV9aokn+ToZyCYxD0KSdLKWu6K4s3dv989dCGSpOnUfI6iqr7QLd4H3F1VnwOeADyPox+ekyStQX0nBdwDnNx9JsWNwOuA9w9VlCRpevQNilTVA8D3A79dVd8HbBuuLEnStOgdFElezOj5if/dbev96XiSpNWrb1C8hdEHDP1ZNw3Hs4GbhitLkjQt+j5H8VHgowvWDzB6+E6StMYt9xzFb1TVW5L8BUs/R7F9sMokSVNhuSuKw3M7/drQhUiSplMzKKrq8MR/+4EHq+rrAEnWMXqeQpK0xvW9mX0j8KQF608E/vbElyNJmjZ9g+LkqvqPwyvd8pMa7SVJa0TfoPjPJC84vJLkhcCDw5QkSZomfR+aewtwXZLD8zs9A3j1MCVJkqZJ3+co9iX5VuBbGH1w0T9W1VcHrUySNBV6DT0leRLwM8Cbq+qTwOYkTj0uSY8BfYee/ojRZ2S/uFufB64D/nKIolq+/nW4666VflWtFU97GszMTLoKaXXpGxTfXFWvTrIDoKoeTJIB62ras2dSr6zV7NChUVDs2DHpSqTVpW9QPJTkiXTTeCT5ZuDQYFU1rF8PZ501iVfWave5z8GXvzzpKqTVp29QvAP4a2BTkj8BXgJcMlRRkqTpsWxQdENM/8joQ4vOYfSupzdX1X0D1yZJmgLLBkVVVZKPVNULOfKhRZKkx4i+T2bvTfIdg1YiSZpKfe9RvAz4kSSfBf6T0fBTVdVzhypMkjQd+gbFBYNWIUmaWst9wt3JwI8AW4BPAldV1cMrUZgkaTosd4/iA8Aso5C4AHj34BVJkqbKckNP26rq2wGSXAX8/+FLkiRNk+WuKB6ZIXacIack5ye5M8lckrc12r0ySSWZPd7XkCQNa7kriucl+fduOcATu/XD73p6yrEO7D5X+0rguxhNIrgvye6qumNRu1OANwEfH/N7kCQNqHlFUVXrquop3dcpVbV+wfIxQ6JzNjBXVQeq6iHgGuCiJdr9IvBO4CtjfQeSpEH1feBuHKcDdy9Yn++2PSLJWcCmqlrx6colSf30fY5iHEtNQ16P7EweB7yHHpMLJtkJ7AQ47bRnnqDyJEl9DHlFMQ9sWrC+EbhnwfopwJnA33VPfJ8D7F7qhnZV7aqq2aqa3bDh6QOWLElabMig2AdsTXJGkpOAi4Hdh3dW1cGqmqmqzVW1GdgLbK+q/QPWJEk6ToMFRfd22suAG4BPA9dW1e1JrkiyfajXlSSdWEPeo6CqrgeuX7Tt7cdoe+6QtUiSxjPk0JMkaQ0wKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkpkFnj5WmzaFDcNddk65CWl0MCj1mbNgA//qvsGfPpCuRJuGUbxj3SINCjxmnngovetGkq5AmZd26cY/0HoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpadCgSHJ+kjuTzCV52xL7fyLJHUluS3JjkmcNWY8k6fgNFhRJ1gFXAhcA24AdSbYtanYLMFtVzwU+DLxzqHokSeMZ8oribGCuqg5U1UPANcBFCxtU1U1V9UC3uhfYOGA9kqQxDBkUpwN3L1if77Ydy6XAXy21I8nOJPuT7D948IsnsERJ0nKGDIossa2WbJi8BpgF3rXU/qraVVWzVTW7YcPTT2CJkqTlrB/w3PPApgXrG4F7FjdK8grg54DvrKpDA9YjSRrDkFcU+4CtSc5IchJwMbB7YYMkZwG/D2yvqnsHrEWSNKbBgqKqHgYuA24APg1cW1W3J7kiyfau2buAJwPXJflEkt3HOJ0kaUKGHHqiqq4Hrl+07e0Lll8x5OtLkh49n8yWJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqWnQoEhyfpI7k8wledsS+5+Q5EPd/o8n2TxkPZKk4zdYUCRZB1wJXABsA3Yk2bao2aXA/VW1BXgP8KtD1SNJGs+QVxRnA3NVdaCqHgKuAS5a1OYi4APd8oeBlyfJgDVJko7TkEFxOnD3gvX5btuSbarqYeAg8I0D1iRJOk7rBzz3UlcGNUYbkuwEdnZrX52dfepnH2Vta8ShDfCEg5OuYjrYF0fYF0fYF0f8+8ZxjxwyKOaBTQvWNwL3HKPNfJL1wAbgy4tPVFW7gF0ASfZX3T87SMWrzKgvHrAvsC8Wsi+OsC+OSLJ/3GOHHHraB2xNckaSk4CLgd2L2uwGfqhbfiXwf6vqqCsKSdLkDHZFUVUPJ7kMuAFYB7yvqm5PcgWwv6p2A1cBVyeZY3QlcfFQ9UiSxjPk0BNVdT1w/aJtb1+w/BXgB47ztLtOQGlrhX1xhH1xhH1xhH1xxNh9EUd6JEktTuEhSWqa2qBw+o8jevTFTyS5I8ltSW5M8qxJ1LkSluuLBe1emaSSrNl3vPTpiySv6n42bk/ypytd40rp8TvyzCQ3Jbml+z25cBJ1Di3J+5Lcm+RTx9ifJL/V9dNtSV7Q68RVNXVfjG5+/zPwbOAk4FZg26I2Pwq8t1u+GPjQpOueYF+8DHhSt/yGx3JfdO1OAfYAe4HZSdc9wZ+LrcAtwFO79dMmXfcE+2IX8IZueRvw2UnXPVBfvBR4AfCpY+y/EPgrRs+wnQN8vM95p/WKwuk/jli2L6rqpqp6oFvdy+iZlbWoz88FwC8C7wS+spLFrbA+ffF64Mqquh+gqu5d4RpXSp++KOAp3fIGjn6ma02oqj0s8SzaAhcBf1wje4FTkzxjufNOa1A4/ccRffpioUsZ/Y9hLVq2L5KcBWyqqr9cycImoM/PxXOA5yT5hyR7k5y/YtWtrD59cTnwmiTzjN6J+WMrU9rUOd6/J8DAb499FE7Y9B9rQO/vM8lrgFngOwetaHKafZHkcYxmIb5kpQqaoD4/F+sZDT+dy+gq8++TnFlV/zZwbSutT1/sAN5fVe9O8mJGz2+dWVVfH768qTLW381pvaI4nuk/aE3/sQb06QuSvAL4OWB7VR1aodpW2nJ9cQpwJvB3ST7LaAx29xq9od33d+TPq+qrVfUZ4E5GwbHW9OmLS4FrAarqY8DJwMyKVDddev09WWxag8LpP45Yti+64ZbfZxQSa3UcGpbpi6o6WFUzVbW5qjYzul+zvarGnuNmivX5HfkIozc6kGSG0VDUgRWtcmX06YvPAy8HSPJtjILiiyta5XTYDby2e/fTOcDBqvrCcgdN5dBTOf3HI3r2xbuAJwPXdffzP19V2ydW9EB69sVjQs++uAE4L8kdwNeAt1bVlyZX9TB69sVPAn+Q5McZDbVcshb/Y5nkg4yGGme6+zHvAB4PUFXvZXR/5kJgDngAeF2v867BvpIknUDTOvQkSZoSBoUkqcmgkCQ1GRSSpCaDQpLUZFBIiyT5WpJPJPlUkr9IcuoJPv8lSX6nW748yU+dyPNLJ5pBIR3twap6flWdyegZnTdOuiBpkgwKqe1jLJg0Lclbk+zr5vL/hQXbX9ttuzXJ1d227+k+K+WWJH+b5JsmUL/0qE3lk9nSNEiyjtG0D1d16+cxmivpbEaTq+1O8lLgS4zm2XpJVd2X5GndKf4fcE5VVZL/Bfw0oyeEpVXFoJCO9sQknwA2AzcD/6fbfl73dUu3/mRGwfE84MNVdR9AVR2enHIj8KFuvv+TgM+sSPXSCebQk3S0B6vq+cCzGP2BP3yPIsAvd/cvnl9VW6rqqm77UnPh/DbwO1X17cAPM5qITlp1DArpGKrqIPAm4KeSPJ7RpHP/M8mTAZKcnuQ04EbgVUm+sdt+eOhpA/Av3fIPIa1SDj1JDVV1S5JbgYur6upuiuqPdbP0/gfwmm6m0l8CPprka4yGpi5h9Klq1yX5F0ZTnp8xie9BerScPVaS1OTQkySpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElN/wU8FCZ5wQIjYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "#plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "         # average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We check how it works by running the best classifier from the grid search on our held out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=1)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x10dd24730>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x1a116508c8>], 'vect__stop_words': ['english', None], 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tfidf__use_idf': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=1)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.6111111111111112,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.658\n",
      "recall: 0.669\n",
      "f1: 0.661\n",
      "fscore_prec: 0.661\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.83      0.83      0.83        47\n",
      "        POS       0.62      0.62      0.62        21\n",
      "\n",
      "avg / total       0.76      0.76      0.76        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6190476190476191\n",
      "0.6190476190476191\n",
      "0.6190476190476191\n",
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english', None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True, False],# (True, False), #DEFAULT\n",
    "    \n",
    "}\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=1.5)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x10dd24730>], 'vect__max_df': array([0.3    , 0.37778, 0.45556, 0.53333, 0.61111, 0.68889, 0.76667,\n",
       "       0.84444, 0.92222, 1.     ]), 'vect__tokenizer': [<function word_tokenize at 0x1a116508c8>], 'vect__stop_words': ['english', None], 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tfidf__use_idf': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=1.5)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.6111111111111112,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.607\n",
      "recall: 0.707\n",
      "f1: 0.652\n",
      "fscore_prec: 0.671\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.79      0.79      0.79        47\n",
      "        POS       0.52      0.52      0.52        21\n",
      "\n",
      "avg / total       0.71      0.71      0.71        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "best_model = svc_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "0.5238095238095238\n",
      "0.5238095238095238\n",
      "0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "scores = [precision_score, recall_score, f1_score,accuracy_score]\n",
    "for s in scores:\n",
    "    score = s(y_test, predictions)\n",
    "    print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
