{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP classifier\n",
    "\n",
    "This notebook trains a Multi-layer Perceptron classifier to identify relevant tweets (POS).\n",
    "\n",
    "We use scikit-learn's implementation and its cross validation tools. http://scikit-learn.org/\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install all of the python dependencies for this notbook in a virtual environment:\n",
    "\n",
    "```bash\n",
    "# create environment in directory named 'venv'\n",
    "python -m venv venv\n",
    "# or:\n",
    "# virtualenv venv\n",
    "\n",
    "# activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# install dependencies\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 1,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "execution_count": 1,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
   "outputs": [],
   "source": [
    "from class_utils import *\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 12,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "execution_count": 12,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
   "outputs": [],
   "source": [
    "# globals\n",
    "model_filename = \"models/best_mlp.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data sets\n",
    "\n",
    "Here we parse data from our training files, and then randomly select a portion to be held out for evaluation. The training set is used to both train the classifier and select parameters using k-fold cross validation.\n",
    "\n",
    "The `parse_training_data()` function is provided in the external `class_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 2,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "execution_count": 2,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
   "outputs": [],
   "source": [
    "# parse data from files\n",
    "classes = ['NEG', 'POS']\n",
    "docs, targets = parse_training_data(['NEG.txt', 'POS.txt'], classes)\n",
    "\n",
    "# convert the targets array of strings to binary labels (0=NEG, 1=POS)\n",
    "lb = LabelBinarizer(sparse_output=False)\n",
    "lb.fit(classes)\n",
    "bin_targets = lb.transform(targets).ravel()\n",
    "\n",
    "# split data set into to training and evaluation sets\n",
    "# X_test/y_test are held out and not used during the\n",
    "# k-fold training and parameter search below\n",
    "#\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "# The percentage of samples to hold out is determined by the `test_size`\n",
=======
    "# The percentage of samples to hod out is determined by the `test_size`\n",
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
    "# The percentage of samples to hod out is determined by the `test_size`\n",
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
    "# parameter\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs, bin_targets, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sklearn pipeline\n",
    "\n",
    "Here we setup a scikit-learn pipeline to create vectors from our training sample vocabulary (`CountVectorizer`), normalize words based on frequency (`TfidfTransformer`), and train a multi-layer perceptron classifier (`MLPClassifier`). http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "We evaluate parameters based on th `fscore_prec` which is a weighted fscore which favors precision (beta < 1). We also calcualte accuracy, precision, recall, and f1 scores for each of the k-fold training sessions.\n",
    "\n",
    "Using a pipeline makes it easy to search a range of hyperparameters using sklearn's `GridSearchCV`. http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 3,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "execution_count": 3,
   "metadata": {},
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
   "outputs": [],
   "source": [
    "mlp_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MLPClassifier(max_iter=1500, warm_start=False)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.4, 1.0, 10),\n",
    "    'vect__tokenizer': [casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words': ['english'], #[None, 'english'],\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': [(1, 2), (1,3)],  # largest n-gram\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__activation': ['relu', 'logistic'],\n",
    "    #'clf__hidden_layer_sizes': [(100,10), (100,)],\n",
    "    'clf__alpha':[.00011] # np.linspace(1e-5, 1e-2, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=0.5)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "mlp_search = GridSearchCV(mlp_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='fscore_prec')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 4,
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "execution_count": 4,
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-271baa872f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here we do the actual training (takes many minutes on my computer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/NLP/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NLP/venv/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NLP/venv/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
=======
=======
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 66.7min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 114.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 189.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 328.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 420.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__preprocessor': [<function normalize_tweet at 0x7f2ea2cc1d90>, <function normalize_simple at 0x7f2ea2cad158>, None], 'vect__max_df': array([0.4    , 0.46667, 0.53333, 0.6    , 0.66667, 0.73333, 0.8    ,\n",
       "       0.86667, 0.93333, 1.     ]), 'vect__tokenizer': [<function casual_tokeni..., 'tfidf__use_idf': [False, True], 'clf__activation': ['relu', 'logistic'], 'clf__alpha': [0.00011]},\n",
       "       pre_dispatch='2*n_jobs', refit='fscore_prec',\n",
       "       return_train_score='warn',\n",
       "       scoring={'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1', 'fscore_prec': make_scorer(fbeta_score, beta=0.5)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
<<<<<<< HEAD
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
    }
   ],
   "source": [
    "# Here we do the actual training (takes many minutes on my computer)\n",
    "mlp_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__activation': 'relu',\n",
       " 'clf__alpha': 0.00011,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__max_df': 0.8666666666666667,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet(item)>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "mlp_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.830\n",
      "precision: 0.778\n",
      "recall: 0.567\n",
      "f1: 0.655\n",
      "fscore_prec: 0.723\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['accuracy', 'precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = mlp_search.cv_results_[\"mean_test_%s\" % f][mlp_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We check how it works by running the best classifier from the grid search on our held out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEG', 'NEG']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try predicting two synthetic tweets using our best model\n",
    "tweet_neg = 'the wise man bowed his head solemnly and spoke: \"theres actually zero difference between good & bad things. you imbecile. you fucking moron\"'\n",
    "tweet_pos = \"thirdgrade parcc test Something about common core and students opting out of testing, assessment, ethnicity\"\n",
    "\n",
    "best_model = mlp_search.best_estimator_\n",
    "pred_class = best_model.predict([tweet_neg, tweet_pos])\n",
    "lb.inverse_transform(pred_class).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.80      0.93      0.86       114\n",
      "        POS       0.75      0.47      0.58        51\n",
      "\n",
      "avg / total       0.78      0.79      0.77       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106   8]\n",
      " [ 27  24]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model\n",
    "\n",
    "Take our best model, retrain it on entire training dataset (including the held out set used for evaluation above), and persist it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8666666666666667, max_features=None,\n",
       "        min_df=1, ngram_range=(1, 3),\n",
       "        preprocessor=<function nor...=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain on all data\n",
    "best_model.fit(docs, bin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-29028b70336e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# save to disk\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< HEAD
   "version": "3.5.2"
=======
   "version": "3.6.5rc1"
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
=======
   "version": "3.6.5rc1"
>>>>>>> 326fdd27d3da41296a7962cf90dd8403b0ce5e7e
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
