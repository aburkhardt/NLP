{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classifier\n",
    "\n",
    "This notebook trains a Support Vector Machine (with a linear kernel) to identify relevant tweets (POS).\n",
    "\n",
    "We use scikit-learn's implementation of SVM and its cross validation tools. http://scikit-learn.org/\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install all of the python dependencies for this notbook in a virtual environment:\n",
    "\n",
    "```bash\n",
    "# create environment in directory named 'venv'\n",
    "python -m venv venv\n",
    "# or:\n",
    "# virtualenv venv\n",
    "\n",
    "# activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# install dependencies\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from class_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "iteration=\"iter3a\"\n",
    "model_filename = \"models/best_svc_{}.pickle\".format(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data sets\n",
    "\n",
    "Here we parse data from our training files, and then randomly select a portion to be held out for evaluation. The training set is used to both train the SVM classifier and select parameters using k-fold cross validation.\n",
    "\n",
    "The `parse_training_data()` function is provided in the external `class_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse data from files\n",
    "classes = ['NEG', 'POS']\n",
    "docs, targets = parse_training_data(['NEG-{}.txt'.format(iteration), 'POS-{}.txt'.format(iteration)], classes)\n",
    "\n",
    "# convert the targets array of strings to binary labels (0=NEG, 1=POS)\n",
    "lb = LabelBinarizer(sparse_output=False)\n",
    "lb.fit(classes)\n",
    "bin_targets = lb.transform(targets).ravel()\n",
    "\n",
    "# split data set into to training and evaluation sets\n",
    "# X_test/y_test are held out and not used during the\n",
    "# k-fold training and parameter search below\n",
    "#\n",
    "# The percentage of samples to hod out is determined by the `test_size`\n",
    "# parameter\n",
    "# for this iter2, the holdout is only going to be 10% \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs, bin_targets, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3       ,  0.37777778,  0.45555556,  0.53333333,  0.61111111,\n",
       "        0.68888889,  0.76666667,  0.84444444,  0.92222222,  1.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.3, 1.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sklearn pipeline\n",
    "\n",
    "Here we setup a scikit-learn pipeline to create vectors from our training sample vocabulary (`CountVectorizer`), normalize words based on frequency (`TfidfTransformer`), and train a SVM classifier (`SVC`). http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "We evaluate parameters based on th `fscore_prec` which is a weighted fscore which favors precision (beta < 1). We also calculate accuracy, precision, recall, and f1 scores for each of the k-fold training sessions.\n",
    "\n",
    "Using a pipeline makes it easy to search a range of hyperparameters using sklearn's `GridSearchCV`. http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pl = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {\n",
    "    'vect__preprocessor': [normalize_tweet],#[normalize_tweet, normalize_simple, None],\n",
    "    'vect__max_df': np.linspace(0.3, 1.0, 10),\n",
    "    'vect__tokenizer': [word_tokenize],#[casual_tokenize, word_tokenize, None],\n",
    "    'vect__stop_words' : ['english',None],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],# ((1, 1), (1, 2), (1,3)),  # largest n-gram\n",
    "    'tfidf__use_idf':[True],# (True, False), #DEFAULT\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "\n",
    "}\n",
    "\n",
    "# define the scores we want to calcualte during each k-fold training\n",
    "fscore_prec = make_scorer(fbeta_score, beta=1)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'fscore_prec': fscore_prec\n",
    "}\n",
    "\n",
    "# create the GridSearchCV object.\n",
    "# by setting refit='fscore_prec', the model which maximizes that score\n",
    "# will be selected and retrained on all training data.\n",
    "svc_search = GridSearchCV(svc_pl, parameters, n_jobs=-1, verbose=1, scoring=scoring, refit='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.9s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'vect__max_df': array([ 0.3    ,  0.37778,  0.45556,  0.53333,  0.61111,  0.68889,\n",
       "        0.76667,  0.84444,  0.92222,  1.     ]), 'vect__preprocessor': [<function normalize_tweet at 0x1062fbd08>], 'tfidf__use_idf': [True], 'vect__tokenizer': [<function word_tokenize at 0x116760158>], 'vect__stop_words': ['english', None]},\n",
       "       pre_dispatch='2*n_jobs', refit='recall', return_train_score='warn',\n",
       "       scoring={'fscore_prec': make_scorer(fbeta_score, beta=1), 'f1': 'f1', 'recall': 'recall', 'precision': 'precision', 'accuracy': 'accuracy'},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the actual training\n",
    "# Can take several minutes depending on the range of parameters given\n",
    "# int he parameters dict above\n",
    "svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.37777777777777777,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': <function class_utils.normalize_tweet>,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__tokenizer': <function nltk.tokenize.word_tokenize>}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameters selected by the grid search\n",
    "svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.730\n",
      "precision: 0.972\n",
      "recall: 0.119\n",
      "f1: 0.206\n",
      "fscore_prec: 0.206\n"
     ]
    }
   ],
   "source": [
    "# print the average scores over the k training folds\n",
    "fields = ['accuracy', 'precision', 'recall', 'f1', 'fscore_prec']\n",
    "\n",
    "for f in fields:\n",
    "    score = svc_search.cv_results_[\"mean_test_%s\" % f][svc_search.best_index_]\n",
    "    print(\"%s: %.3f\" % (f, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We check how it works by running the best classifier from the grid search on our held out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get best model from grid search we ran in previous section\n",
    "best_model = svc_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use model to predict held out set (X_test) and print score table\n",
    "# Note that in binary classification, accuracy is the same as the\n",
    "# [mico averaged recall reported in the table\n",
    "predictions = best_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model\n",
    "\n",
    "Take our best model, retrain it on entire training dataset (including the held out set used for evaluation above), and persist it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrain on all data\n",
    "best_model.fit(docs, bin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
