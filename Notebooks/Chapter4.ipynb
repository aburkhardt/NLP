{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Chapter 4\n",
    "## Smoothing motivation\n",
    "\n",
    "If a word appears in the test set in a context different than any it appears in the training set, then the n-gram model will assign it a zero probability even if it is a likely word. To fix this, we want our model to give words in novel contexts a non-zero probability either by pretending that it occured in the training set (increase its count by 1, a la Laplace smoothing) or by considering it in a less-specific context (via interpolation or the various backoff strategies).\n",
    "\n",
    "But we want our n-gram model to provide a probability mass function -- all of the probabilities should sum to 1 -- so if we increase the probability of novel n-grams then we need to decrease the probability of frequent n-grams. This is what the text means by \"discounting\": \"To keep a language model from assigning zero probability to these unseen events, we’ll have to shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen.\"\n",
    "\n",
    "So we \"smooth\" the probability mass function: novel n-grams are made more probable and frequent n-grams less probable, with the sum of all probabilities = 1.\n",
    "\n",
    "However, it's possible we don't care if it is a true probability distribution. That's the the point of the \"Stupid Backup\" strategy: it's probabilities don't sum to 1, but it is simple because it doesn't have to worry about discounting.\n",
    "\n",
    "## Notation\n",
    "\n",
    "Many of the formulas are given without much in the way of explanation -- did this cause some of the confusion in class?\n",
    "\n",
    "I was confused at first by this bit of equation 4-26:\n",
    "\n",
    "$$\\lambda_n \\left(w_{n-2}^{n-1}\\right)$$\n",
    "\n",
    "I thought it was multiplying lambda by a sequence of words and didn't know what that meant. But the parentheses in this case signify a paramter: so it means the lambda value for the given bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excerices 4.8-4.11\n",
    "\n",
    "## 4.8\n",
    "\n",
    "> Write a program to compute unsmoothed unigrams and bigrams.\n",
    "\n",
    "Two possible implementations come to mind.\n",
    "\n",
    "One is to store n-grams in a [trie structure](https://en.wikipedia.org/wiki/Trie) where each node represents a word and contains both a count and a `children` dict whose keys are strings and whose values are nodes representing words which have followed the current node in an n-gram. So this sentence:\n",
    "\n",
    "    \"<s>a b a c</s>\"\n",
    "\n",
    "would become a forest of bigram trees (where the count is in parentheses):\n",
    "\n",
    "    <s>(1)     a(2)     b(1)  c(1)\n",
    "      |       /   \\      |     |\n",
    "     a(1)   b(1)   c(1) a(1) </s>(1)\n",
    "\n",
    "The top-level nodes are the unigrams, the second level nodes following their parent are the bigrams, and so on.\n",
    "\n",
    "The other implementation I thought of is to build a dictionary of whose keys are n-tuples of strings representing the n-gram and whose values are the count of times that n-gram appeared in the training text. We could then also build an n-1-gram dict, an n-2-gram dict... and a unigram dict, and store them all in a dict keyed by `n`:\n",
    "\n",
    "````python\n",
    "ngrams_dict = {\n",
    "    1: { (word1,): 5,          # unigrams\n",
    "         (word2,): 4\n",
    "       },\n",
    "    2: { (word1, word2): 3,    # bigrams\n",
    "         (word2, word3): 2\n",
    "       },\n",
    "    ...,\n",
    "    n: ...                     # n-grams\n",
    "}\n",
    "````\n",
    "\n",
    "The trie would take less memory which is an important consideration for very large text corpuses. But the dict-of-ngrams method seems easier to implement so I decided to go with that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import random\n",
    "from math import log2\n",
    "SENT_START = \"<s>\"\n",
    "SENT_END = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    Class representing an n-gram model of a language.\n",
    "    \n",
    "    Initialize it with some sentences and a degree (default=2, for bigrams),\n",
    "    and it will count all n-grams, n-1-grams ... unigrams.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, n=2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sentences: a list of sentences (each sentence a list of words);\n",
    "                       SENT_START and SENT_END sentinals are added before counting n-grams\n",
    "                       \n",
    "            n: an integer specifying the highest-order n-grams to count\n",
    "        \n",
    "        More sentences can be added to the model after initialization by callig add_sentences()\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngram_dict = {}\n",
    "        self.add_sentences(sentences) \n",
    "    \n",
    "    def add_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sentences: a list of sentences (each sentence a list of words);\n",
    "                       SENT_START and SENT_END sentinals are added before counting n-grams\n",
    "        \"\"\"\n",
    "        # build a list of the nth-order ngrams\n",
    "        ngrams = []\n",
    "        for sentence in sentences:\n",
    "            # Add sentinal symbols to start and end of each sentence\n",
    "            # we add n-number of symbols so that the sentence ['a', 'b', 'c'] becomes\n",
    "            # ['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>'] for n=2\n",
    "            # \n",
    "            # See \"Some practical issues\" on pages 6-7 of the text.\n",
    "            n = self.n\n",
    "            start_tokens = [SENT_START]*n\n",
    "            end_tokens = [SENT_END]*n\n",
    "            sentence[0:0] = start_tokens\n",
    "            sentence.extend(end_tokens)\n",
    "            length = len(sentence)\n",
    "            for i in range(length):\n",
    "                # from through the words in sentence left-to-right taking n at a time\n",
    "                span = i+n\n",
    "                if span > length:\n",
    "                    # got to the end, move to next sentence\n",
    "                    break\n",
    "                ngrams.append(tuple(sentence[i:span]))\n",
    "        \n",
    "        # build ngrams dict\n",
    "        # This does one thing clever: it derives all the lower-order n-grams from\n",
    "        # the n-gram tuples instead of processing the sentences a second time.\n",
    "        #\n",
    "        # To create a bigram from ('word1', 'word2', 'word3') for example, just remove the last component.\n",
    "        for i in reversed(range(n)):\n",
    "            degree = i+1\n",
    "            lower = n - degree\n",
    "            degn_grams = [ngram[0:n-lower] for ngram in ngrams]\n",
    "            counts = Counter(degn_grams)\n",
    "            \n",
    "            # Now merge these counts with the existing counts in self.ngram_dict[degree]\n",
    "            for k, v in counts.items():\n",
    "                try:\n",
    "                    # make sure the degree key exists\n",
    "                    self.ngram_dict[degree]\n",
    "                except KeyError:\n",
    "                    self.ngram_dict[degree] = {}\n",
    "                try:\n",
    "                    # if the key for this ngram doesn't exist, then create it\n",
    "                    self.ngram_dict[degree][k] += v\n",
    "                except KeyError:\n",
    "                    self.ngram_dict[degree][k] = v\n",
    "        return ngrams\n",
    "    \n",
    "    @classmethod\n",
    "    def ngram_to_tuple(cls, ngram):\n",
    "        \"\"\"\n",
    "        Helper method to turn strings and lists into tuples.\n",
    "        \"\"\"\n",
    "        if isinstance(ngram, str):\n",
    "            # if we were passed a bare string, make it a list\n",
    "            ngram = ngram.split()\n",
    "        if isinstance(ngram, list):\n",
    "            # if we were passed a list, make it a tuple\n",
    "            ngram = tuple(ngram)\n",
    "        return ngram\n",
    "    \n",
    "    def ngrams(self, n=None):\n",
    "        \"\"\"\n",
    "        Return a list of n-grams of order n\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            # default to the n we were initialized with\n",
    "            n = self.n\n",
    "        return self.ngram_dict[n].keys()\n",
    "    \n",
    "    def count(self, ngram):\n",
    "        \"\"\"\n",
    "        Returns the count for ngram or None if ngram does not exist in model.\n",
    "        \"\"\"\n",
    "        ngram = self.ngram_to_tuple(ngram)\n",
    "        order = len(ngram)\n",
    "        if order > self.n:\n",
    "            raise ValueError(\"The order of the given ngram exceeds the order of this model.\")\n",
    "        return self.ngram_dict[order].get(ngram, None)\n",
    "    \n",
    "    def prob(self, ngram):\n",
    "        \"\"\"\n",
    "        Return the MLE of the probability for ngram.\n",
    "        Unsmoothed (if ngram doesn't exist in the model, its probability is 0)\n",
    "        \"\"\"\n",
    "        ngram = self.ngram_to_tuple(ngram)\n",
    "        count = self.count(ngram)\n",
    "        if count is None: return 0\n",
    "        \n",
    "        # get the count of the n-1th word in the n-gram\n",
    "        n_minus_one = len(ngram)-1\n",
    "        if n_minus_one == 0:\n",
    "            # for unigrams, we get the count of all word types:\n",
    "            pcount = sum(self.ngram_dict[1].values())\n",
    "        else:\n",
    "            pcount = self.count(ngram[0:n_minus_one])\n",
    "        \n",
    "        return count/pcount\n",
    "    \n",
    "    def top(self, x=10, n=None, prob=None):\n",
    "        \"\"\"\n",
    "        Return a list of tuples containing the x most frequent n-grams\n",
    "        \"\"\"\n",
    "        if n is None: n = self.n\n",
    "        if prob is None: prob = self.prob\n",
    "\n",
    "        counts = self.ngram_dict[n]\n",
    "        return sorted(counts, key=counts.get, reverse=True)[0:x]\n",
    "\n",
    "    def format_top(self, x=10, n=None, prob=None):\n",
    "        \"\"\"\n",
    "        Returns a string of the x most frequent n-grams, their counts (c) and their probabilities (p)\n",
    "        \"\"\"\n",
    "        if prob is None: prob = self.prob\n",
    "        top_list = self.top(x=x, n=n, prob=prob)\n",
    "        \n",
    "        string = ''\n",
    "        for ngram in top_list:\n",
    "            c = self.count(ngram)\n",
    "            p = prob(ngram)\n",
    "            string += \"%s: c=%d, p=%f\\n\" % (ngram, c, p)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how the `NGramModel` class works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'Sam'], ['Sam', 'I', 'am'], ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']]\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    [*\"I am Sam\".split()],\n",
    "    [*\"Sam I am\".split()],\n",
    "    [*\"I do not like green eggs and ham\".split()]\n",
    "]\n",
    "print(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('Sam', '</s>'), ('am', 'Sam'), ('<s>', '<s>'), ('do', 'not'), ('am', '</s>'), ('<s>', 'Sam'), ('not', 'like'), ('</s>', '</s>'), ('Sam', 'I'), ('<s>', 'I'), ('I', 'am'), ('and', 'ham'), ('eggs', 'and'), ('like', 'green'), ('ham', '</s>'), ('I', 'do'), ('green', 'eggs')])\n"
     ]
    }
   ],
   "source": [
    "bigram_model = NGramModel(example_sentences, 2)\n",
    "bigrams = bigram_model.ngrams(2) # get a list of all bigrams\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many times the bigram 'I am' appeared in training text\n",
    "bigram_model.count(['I', 'am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the MLE probability that the bigram the word 'I' will follow the word 'Sam' in the language\n",
    "bigram_model.prob(['Sam', 'I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9\n",
    "\n",
    "> Run your N-gram program on two different small corpora of your choice (you\n",
    "might use email text or newsgroups). Now compare the statistics of the two\n",
    "corpora. What are the differences in the most common unigrams between the\n",
    "two? How about interesting differences in bigrams?\n",
    "\n",
    "We'll use some of the example corpuses which come with NLTK. They are already tokenized and segmented into sentences. See http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "hamlet = nltk.corpus.gutenberg.sents('shakespeare-hamlet.txt')\n",
    "chesterton = nltk.corpus.gutenberg.sents('chesterton-thursday.txt')\n",
    "bigram_hamlet = NGramModel(hamlet, 2)\n",
    "bigram_chester = NGramModel(chesterton, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print the top 20 unigrams and bigrams with counts (c) and probabilities (p) from each corpus. The punctuation and sentence markers sort of get in the way. The 20th most frequent unigram in *The Man Who Was Thursday* is a character's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigrams, their counts, and probabilities from Hamlet:\n",
      "('<s>',): c=6212, p=0.133082\n",
      "('</s>',): c=3106, p=0.066541\n",
      "(',',): c=2892, p=0.061956\n",
      "('.',): c=1886, p=0.040404\n",
      "('the',): c=860, p=0.018424\n",
      "(\"'\",): c=729, p=0.015618\n",
      "('and',): c=606, p=0.012983\n",
      "('to',): c=576, p=0.012340\n",
      "('of',): c=576, p=0.012340\n",
      "(':',): c=565, p=0.012104\n",
      "('I',): c=553, p=0.011847\n",
      "('you',): c=479, p=0.010262\n",
      "('?',): c=459, p=0.009833\n",
      "('a',): c=449, p=0.009619\n",
      "('my',): c=435, p=0.009319\n",
      "('in',): c=359, p=0.007691\n",
      "('it',): c=354, p=0.007584\n",
      "('Ham',): c=337, p=0.007220\n",
      "('is',): c=304, p=0.006513\n",
      "(';',): c=298, p=0.006384\n",
      "\n",
      "\n",
      "Top 20 unigrams, their counts, and probabilities from The Man Who Was Thursday:\n",
      "('<s>',): c=7484, p=0.093039\n",
      "('</s>',): c=3742, p=0.046520\n",
      "(',',): c=3488, p=0.043362\n",
      "('the',): c=3291, p=0.040913\n",
      "('.',): c=2717, p=0.033777\n",
      "('a',): c=1713, p=0.021296\n",
      "('of',): c=1710, p=0.021258\n",
      "('and',): c=1568, p=0.019493\n",
      "('\"',): c=1336, p=0.016609\n",
      "('to',): c=1045, p=0.012991\n",
      "('in',): c=888, p=0.011039\n",
      "('I',): c=885, p=0.011002\n",
      "('he',): c=858, p=0.010666\n",
      "('that',): c=841, p=0.010455\n",
      "('his',): c=765, p=0.009510\n",
      "('was',): c=716, p=0.008901\n",
      "('you',): c=580, p=0.007210\n",
      "('it',): c=565, p=0.007024\n",
      "('with',): c=544, p=0.006763\n",
      "('Syme',): c=515, p=0.006402\n",
      "\n",
      "\n",
      "Top 20 bigrams, their counts, and probabilities from Hamlet:\n",
      "('<s>', '<s>'): c=3106, p=0.500000\n",
      "('</s>', '</s>'): c=3106, p=1.000000\n",
      "('.', '</s>'): c=1879, p=0.996288\n",
      "('?', '</s>'): c=459, p=1.000000\n",
      "('Ham', '.'): c=337, p=1.000000\n",
      "('<s>', 'Ham'): c=337, p=0.054250\n",
      "(',', 'and'): c=305, p=0.105463\n",
      "(\"'\", 'd'): c=223, p=0.305898\n",
      "(',', 'And'): c=161, p=0.055671\n",
      "('<s>', 'I'): c=157, p=0.025274\n",
      "('my', 'Lord'): c=146, p=0.335632\n",
      "(\"'\", 's'): c=122, p=0.167353\n",
      "(',', 'I'): c=105, p=0.036307\n",
      "('King', '.'): c=96, p=0.558140\n",
      "('Hor', '.'): c=95, p=1.000000\n",
      "('<s>', 'Hor'): c=93, p=0.014971\n",
      "('<s>', 'King'): c=90, p=0.014488\n",
      "(\"'\", 't'): c=84, p=0.115226\n",
      "('<s>', 'Enter'): c=80, p=0.012878\n",
      "('<s>', 'What'): c=79, p=0.012717\n",
      "\n",
      "\n",
      "Top 20 bigrams, their counts, and probabilities from The Man Who Was Thursday:\n",
      "('</s>', '</s>'): c=3742, p=1.000000\n",
      "('<s>', '<s>'): c=3742, p=0.500000\n",
      "('.', '</s>'): c=2595, p=0.955098\n",
      "('<s>', '\"'): c=997, p=0.133218\n",
      "(',', 'and'): c=569, p=0.163131\n",
      "('of', 'the'): c=413, p=0.241520\n",
      "('.\"', '</s>'): c=391, p=1.000000\n",
      "(',', '\"'): c=286, p=0.081995\n",
      "('<s>', 'The'): c=285, p=0.038081\n",
      "(',\"', 'said'): c=272, p=0.536489\n",
      "('<s>', 'He'): c=247, p=0.033004\n",
      "('in', 'the'): c=226, p=0.254505\n",
      "(\"'\", 's'): c=224, p=0.451613\n",
      "('?\"', '</s>'): c=203, p=1.000000\n",
      "('\"', 'I'): c=201, p=0.150449\n",
      "('<s>', 'I'): c=179, p=0.023918\n",
      "(',', 'but'): c=159, p=0.045585\n",
      "('!\"', '</s>'): c=158, p=1.000000\n",
      "('with', 'a'): c=143, p=0.262868\n",
      "('<s>', 'But'): c=137, p=0.018306\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 unigrams, their counts, and probabilities from Hamlet:\")\n",
    "print(bigram_hamlet.format_top(x=20, n=1))\n",
    "print()\n",
    "\n",
    "print(\"Top 20 unigrams, their counts, and probabilities from The Man Who Was Thursday:\")\n",
    "print(bigram_chester.format_top(x=20, n=1))\n",
    "print()\n",
    "\n",
    "print(\"Top 20 bigrams, their counts, and probabilities from Hamlet:\")\n",
    "print(bigram_hamlet.format_top(x=20, n=2))\n",
    "print()\n",
    "\n",
    "print(\"Top 20 bigrams, their counts, and probabilities from The Man Who Was Thursday:\")\n",
    "print(bigram_chester.format_top(x=20, n=2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10\n",
    "\n",
    "> Add an option to your program to generate random sentences.\n",
    "\n",
    "This is the fun part! We'll actually add three new methods to the NGramModel class:\n",
    "\n",
    "- **continue_all** when given a context (a list of words whose length is less than n) will return all the possible next words\n",
    "\n",
    "- **continue_prob** when given a context (a list of words whose length is less than n) will return a random word based on the probability model of the language\n",
    "\n",
    "- **random_sentence** repeatedly calls continue_prob() to generate a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extend our NGramModel with some more functions\n",
    "class NGramModel(NGramModel):\n",
    "    \n",
    "    def continue_all(self, ngram):\n",
    "        \"\"\"\n",
    "        Return a list of possible words following the words in the tuple ngram.\n",
    "        \n",
    "        For example, if our model is a trigram model, we can give this function a bigram\n",
    "        (or unigram) and it will return all possible words which could follow. This takes\n",
    "        words which never occured in the context of ngram as having a 0 probability of occurring\n",
    "        (unsmoothed)\n",
    "        \"\"\"\n",
    "        ngram = self.ngram_to_tuple(ngram)\n",
    "        order = len(ngram)+1\n",
    "        if order > self.n:\n",
    "            raise ValueError(\"The order of the given ngram exceeds the order of this model.\")\n",
    "        \n",
    "        results = []\n",
    "        for candidate in self.ngram_dict[order].keys():\n",
    "            short = candidate[0:-1]\n",
    "            if short == ngram:\n",
    "                results.append(candidate[-1])\n",
    "        return results\n",
    "    \n",
    "    def continue_prob(self, ngram):\n",
    "        \"\"\"\n",
    "        Give a random continuation to ngram\n",
    "        \n",
    "        Given an ngram of order less than n as context, randomly produce the next word according\n",
    "        to the probability model.\n",
    "        \"\"\"\n",
    "        ngram = self.ngram_to_tuple(ngram)\n",
    "        order = len(ngram)+1\n",
    "        if order > self.n:\n",
    "            raise ValueError(\"The order of the given ngram exceeds the order of this model.\")\n",
    "        \n",
    "        # generate a uniform random number [0, 1)\n",
    "        rand = random()\n",
    "        \n",
    "        # variable to hold the accumulated probability\n",
    "        acc = 0\n",
    "        \n",
    "        # Get a list of possible continuations... in a smoothed model\n",
    "        # we would have to instead iterate over all ngrams\n",
    "        possible = self.continue_all(ngram)\n",
    "        \n",
    "        # We iterate over all the possible words, accumulating their probabilities\n",
    "        # until the accumulated probability is greater than the random number\n",
    "        # This will produce a random word distributed according to our model's probability mass\n",
    "        for word in possible:\n",
    "            acc += self.prob(ngram + (word,))\n",
    "            if rand < acc:\n",
    "                return word\n",
    "\n",
    "    def random_sentence(self):\n",
    "        \"\"\"\n",
    "        Repeatedly calls continue_prob() to generate a random sentence.\n",
    "        \"\"\"\n",
    "        words = [SENT_START] * (self.n-1)\n",
    "        next_word = None\n",
    "        while next_word != SENT_END:\n",
    "            next_word = self.continue_prob(words[-self.n+1:])\n",
    "            words.append(next_word)\n",
    "            \n",
    "        # remove start and end of sentence tokens:\n",
    "        words = [word for word in words[:-1] if word != SENT_START]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it by generating 5 random sentences from trigram models based on the two corpuses we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_hamlet = NGramModel(hamlet, 3)\n",
    "trigram_chester = NGramModel(chesterton, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentences based on Hamlet:\n",
      "\n",
      "0: Affection , puh .\n",
      "1: Ham .\n",
      "2: To sound what stop she please .\n",
      "3: Thy selfe do grace to them : There with fantasticke Garlands did she come , it courses through The naturall Gates and Allies of the knee , Where in necessitie of matter Beggard , Will want true colour ; teares perchance for blood\n",
      "4: Exeunt\n",
      "\n",
      "Random sentences based on The Man Who Was Thursday:\n",
      "\n",
      "0: The outer ring -- the main mass of men under the influence of Syme again grew black with supernatural terrors .\n",
      "1: \" We might have been one to the blue and buttons !\n",
      "2: \" Comrades ,\" he said , \" what are you not pull my nose ?\"\n",
      "3: \" Oh , bring me some day .\"\n",
      "4: He read the message --\n"
     ]
    }
   ],
   "source": [
    "print(\"Random sentences based on Hamlet:\\n\")\n",
    "for i in range(5):\n",
    "    print(\"%d: \" % i + trigram_hamlet.random_sentence())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Random sentences based on The Man Who Was Thursday:\\n\")\n",
    "for i in range(5):\n",
    "    print(\"%d: \" % i + trigram_chester.random_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11\n",
    "\n",
    "> Add an option to your program to compute the perplexity of a test set.\n",
    "\n",
    "We add a method `perplexity()` based on this formula:\n",
    "\n",
    "$$exp\\left({\\frac{1}{N}\\sum_N \\log \\left(\\frac{1}{p(w_i\\vert w_{i-1})}\\right)}\\right)$$\n",
    "\n",
    "Except we use log base 2 instead of natural log. Note that this is the same as equation 4.16 in the text, except it uses a sum of logprobs instead of multiplying probabilities (like the text itself recommends doing elsewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramModel(NGramModel):\n",
    "    def perplexity(self, ngram_iter):\n",
    "        \"\"\"\n",
    "        Takes an iterable of n-tuples representing n-grams and calculates the model perplexity.\n",
    "        \"\"\"\n",
    "        sum_of_probs = 0\n",
    "        N = 0\n",
    "        for ngram in ngram_iter:\n",
    "            N += len(ngram)\n",
    "            sum_of_probs += log2(1/self.prob(ngram))\n",
    "        avg = sum_of_probs / N\n",
    "        return 2**avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase we haven't handled out of vocabulary words in our corpus, and we haven't implemented smoothing so that even known words in new contexts have a probability of 0, we can't really apply our perplexity measure to a real corpus (any unknown n-gram would cause a divide by zero).\n",
    "\n",
    "But we can give it a few known n-grams to see that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.44134664616274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_chester = NGramModel(chesterton, 2)\n",
    "bigram_chester.perplexity([['<s>', 'is'], ['there', 'his']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm actually not sure if it is working correctly. And the text says about calculating perplexity:\n",
    "\n",
    "> Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers `<s>` and `</s>` in the probability computation. We also need to include the end-of-sentence marker `</s>` (but not the beginning-of-sentence marker `<s>`) in the total count of word tokens N.\n",
    "\n",
    "I guess it says not to count `<s>` because the probability that the next word in the sentence could be the first word in the sentence is 0... but I am currently counting `<s>` in both `perplexity` and in the model probabilities. I'm not sure what the ramifications are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
