{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "return \n",
    "- training data\n",
    "- intermediate data\n",
    "- test data\n",
    "- sentence to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/POS-training.txt\",'\\t', \n",
    "                      header=None, \n",
    "                      skip_blank_lines=False, \n",
    "                      keep_default_na = False,\n",
    "                      names = ['word_Num', 'word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sent = ['the', 'dog', 'ate', 'the', 'food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fixed Vocabulary and Tag Lists\n",
    "return \n",
    "- tag list\n",
    "- vocabuarly list called vocabulary\n",
    "- events list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = ['CC', 'CD',\n",
    "        'DT',\n",
    "        'EX',\n",
    "        'FW',\n",
    "        'IN', \n",
    "        'JJ', 'JJR', 'JJS',\n",
    "        'LS', \n",
    "        'MD',\n",
    "        'NN', 'NNS', 'NNP', 'NNPS',\n",
    "        'PDT', 'POS', 'PRP', 'PRP$',\n",
    "        'RB', 'RBR', 'RBS', 'RP',\n",
    "        'SYM', \n",
    "        'TO', \n",
    "        'UH', \n",
    "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "        'WDT', 'WP', 'WP$', 'WRB', \n",
    "        '$', '#', '\"', '(', ')', ',', '.', ':'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_dict(data, ngrams = \"tag_word\"):\n",
    "    \"\"\"\n",
    "    Creates dict of ngrams (key) and count (value). \n",
    "    \n",
    "    Arguments: \n",
    "        data: DataFrame with 'tag' and 'word' colum\n",
    "        negrams: denote type of ngram (unigram or bigram) and if want words or tags: word_word or tag_word\n",
    "    Returns:\n",
    "        A dict where key is either a unigram or a bigram tuple, and value is the count of the ngrams\n",
    "    \"\"\"\n",
    "    if ngrams == \"tag_tag\":     \n",
    "        col_1 = data['tag']\n",
    "        col_2 = col_1[1:col_1.shape[0]]\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "        ngram_count[('', col_1[0])] += 1\n",
    "\n",
    "    \n",
    "    if ngrams == \"tag_word\": # not really bi-grams, just getting words and tags\n",
    "        col_1 = data['word']\n",
    "        col_2 = data['tag']\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "            \n",
    "    if ngrams == 'tag': \n",
    "        ngram_count = dict(Counter(data.tag))      \n",
    "        \n",
    "    if ngrams == 'word': \n",
    "        ngram_count = dict(Counter(data.word))\n",
    "            \n",
    "    return ngram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fixed vocabulary; identify which words will be considered UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words that we will call unknowns, and replace these instances in the dataframe\n",
    "unigrams = ngram_dict(train, \"word\")\n",
    "unknowns = { key:value for key, value in unigrams.items() if value < 3 }\n",
    "unknowns = unknowns.fromkeys(unknowns, 'UNK')\n",
    "# replace words that appear less than three times with UNK in training data\n",
    "train['word'] = train['word'].replace(unknowns)\n",
    "#get list of vocabulary \n",
    "vocab = ngram_dict(train, \"word\")\n",
    "vocabulary = list(vocab.keys())\n",
    "vocabulary.remove('') # remove spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return index of the words in the new sentece from the fixed vocabuarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "dog\n",
      "ate\n",
      "the\n",
      "food\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[561, 675, 675, 561, 617]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = []\n",
    "for word in new_sent:\n",
    "    print(word)\n",
    "    try: \n",
    "        events.append(vocabulary.index(word))\n",
    "    except: events.append(vocabulary.index('UNK'))\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Transition and Observation Matrices\n",
    "return\n",
    "-tran matrix\n",
    "-observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_transition_matrix (tags, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the transition matrix (len(tags)+1 x len(tags))\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        bigram_counts: count of bigrams of POS tags in training data (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: 45 x 44 matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    transition = [] # list of transition probabilities \n",
    "    \n",
    "    # first compute the starting probabilities \n",
    "\n",
    "    for x in tags: \n",
    "            pair = ('',x) # The space denotes the start of a sentence\n",
    "            denominator = unigram_counts[''] + len(tags)\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                 numerator = 1\n",
    "            transition.append(numerator / denominator)\n",
    "\n",
    "\n",
    "    # then compute everything else \n",
    "    \n",
    "    for x in tags:\n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try:\n",
    "                denominator = unigram_counts[x] + len(tags) \n",
    "            except: \n",
    "                denominator = len(tags)\n",
    "            try: \n",
    "                numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                numerator = 1 \n",
    "            transition.append(numerator / denominator)\n",
    "   \n",
    "    \n",
    "    transition = np.array(transition)\n",
    "    tran_matrix = transition.reshape(len(tags)+1, len(tags))\n",
    "    \n",
    "    return tran_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tag_counts = ngram_dict(train, \"tag_tag\")\n",
    "unigram_tag_counts = ngram_dict(train, \"tag\")\n",
    "transitions = compute_transition_matrix (tags, bigram_tag_counts, unigram_tag_counts)\n",
    "np.sum(transitions, axis = 1) # confirm that most rows sum closely to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00006692,  1.        ,  1.        ,  0.99666954,  1.        ,\n",
       "        0.96741855,  1.        ,  0.99540975,  1.        ,  1.        ,\n",
       "        0.83018868,  1.        ,  0.98220943,  0.99804061,  0.97260274,\n",
       "        1.        ,  1.        ,  1.        ,  0.99967685,  1.        ,\n",
       "        0.99453552,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.99934645,  1.        ,  1.        ,\n",
       "        0.9908046 ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.00294413,  1.        ])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_observation_matrix (tags, vocabulary, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the observation matrix (tags, vocabulary)\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        vocabulary: words that appear in the training set. Any words that appear less than 2 times = UNK\n",
    "        bigram_counts: count of bigrams of (tag, word) (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: len(tags) x len(vocabulary) matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    observations = [] # list of observation likelihoods\n",
    "    for x in vocabulary: \n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try: \n",
    "                denominator = unigram_counts[y]\n",
    "            except: \n",
    "                 denominator = 1\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair]\n",
    "            except: numerator = 0\n",
    "            observations.append(numerator / denominator)\n",
    "            \n",
    "    observations = np.array(observations)\n",
    "    obs_matrix = observations.reshape(len(tags),len(vocabulary))  \n",
    "    return obs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tag_counts = ngram_dict(train, \"tag_tag\")\n",
    "unigram_tag_counts = ngram_dict(train, \"tag\")\n",
    "transitions = compute_transition_matrix (tags, bigram_tag_counts, unigram_tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigram_counts = ngram_dict(train, \"tag_word\")\n",
    "unigram_counts = ngram_dict(train, \"tag\")\n",
    "observations = compute_observation_matrix(tags, vocabulary, bigram_counts, unigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm\n",
    "return\n",
    "-predicted POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi (transition, observations, events):\n",
    "    \"\"\" Computes sequnce of hidden states, given observed events.\n",
    "    Arguments: \n",
    "        transition: transition matrix with start probabilites as first row\n",
    "        observations: observation liklihood matrix, with states as rows, and vocabulary as columns\n",
    "        events: sequence of observed events\n",
    "        \n",
    "    Returns: \n",
    "        generator, which yields the states\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition.shape[1]\n",
    "    n_events = len(events)\n",
    "    v = np.zeros((n_states, n_events))\n",
    "    bp = v.copy()\n",
    "    \n",
    "    # initialization step\n",
    "    for s in range(n_states):\n",
    "        v[s,0] = tran[0,s] * observations[s, events[0]-1]\n",
    "\n",
    "    # induction step\n",
    "    for t in range (1, n_events):\n",
    "        for s in range(n_states):\n",
    "            tmp = []\n",
    "            for s_prime in range (n_states): \n",
    "                prev_t = v[s_prime, t-1]\n",
    "                tran_s_prime_to_s = tran[s_prime + 1, s]\n",
    "                obser_s_given_t = observations[s, events[t]-1]\n",
    "                tmp.append(prev_t * tran_s_prime_to_s *obser_s_given_t)\n",
    "            # now that all interim probabilities have been computed for given state, get max\n",
    "            # and also store the index of the argmax\n",
    "            v[s,t] = max(tmp)\n",
    "            bp[s,t] = np.argmax(tmp)\n",
    "\n",
    "    # termination step\n",
    "    q = np.argmax(v[:, n_events-1]) # want to get the argmax of the final time -- it will return a state index\n",
    "\n",
    "    # back reference step \n",
    "    for i in reversed(range(n_events)):\n",
    "        yield q\n",
    "        q = int(bp[q,i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(viterbi_gen, names_events):\n",
    "    \"\"\" translate viterbi generater into a sequence of state anme\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for state in viterbi_gen:\n",
    "        name = names_events[state]\n",
    "        sequence.insert(0, name)\n",
    "        \n",
    "    return(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ice_cream = viterbi(transitions, observations, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC', 'CC', 'CC', 'CC', 'CC']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence(ice_cream, tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
