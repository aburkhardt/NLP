{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "return \n",
    "- training data\n",
    "- intermediate data\n",
    "- test data\n",
    "- sentence to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/rand_training.txt\",'\\t', \n",
    "                      header=None, \n",
    "                      skip_blank_lines=False, \n",
    "                      keep_default_na = False,\n",
    "                      names = ['word_Num', 'word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_Num</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>'d</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>like</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>go</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>to</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>fancy</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>next</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>thursday</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>dinner</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>i</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>want</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>eat</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>french</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>food</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>far</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>away</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133221</th>\n",
       "      <td>2</td>\n",
       "      <td>thai</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133222</th>\n",
       "      <td>3</td>\n",
       "      <td>cuisine</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133223</th>\n",
       "      <td>4</td>\n",
       "      <td>yorkshire</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133224</th>\n",
       "      <td>5</td>\n",
       "      <td>fish</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133225</th>\n",
       "      <td>6</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133226</th>\n",
       "      <td>7</td>\n",
       "      <td>chips</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133227</th>\n",
       "      <td>8</td>\n",
       "      <td>zachary</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133228</th>\n",
       "      <td>9</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133229</th>\n",
       "      <td>10</td>\n",
       "      <td>chicago</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133230</th>\n",
       "      <td>11</td>\n",
       "      <td>yorkshire</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133231</th>\n",
       "      <td>12</td>\n",
       "      <td>fish</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133232</th>\n",
       "      <td>13</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133233</th>\n",
       "      <td>14</td>\n",
       "      <td>chips</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133234</th>\n",
       "      <td>15</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133235</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133236</th>\n",
       "      <td>1</td>\n",
       "      <td>won</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133237</th>\n",
       "      <td>2</td>\n",
       "      <td>thai</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133238</th>\n",
       "      <td>3</td>\n",
       "      <td>cuisine</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133239</th>\n",
       "      <td>4</td>\n",
       "      <td>yorkshire</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133240</th>\n",
       "      <td>5</td>\n",
       "      <td>fish</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133241</th>\n",
       "      <td>6</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133242</th>\n",
       "      <td>7</td>\n",
       "      <td>chips</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133243</th>\n",
       "      <td>8</td>\n",
       "      <td>zachary</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133244</th>\n",
       "      <td>9</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133245</th>\n",
       "      <td>10</td>\n",
       "      <td>chicago</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133246</th>\n",
       "      <td>11</td>\n",
       "      <td>yorkshire</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133247</th>\n",
       "      <td>12</td>\n",
       "      <td>fish</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133248</th>\n",
       "      <td>13</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133249</th>\n",
       "      <td>14</td>\n",
       "      <td>chips</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133250</th>\n",
       "      <td>15</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133251 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_Num        word  tag\n",
       "0             1           i  PRP\n",
       "1             2          'd   MD\n",
       "2             3        like   VB\n",
       "3             4          to   TO\n",
       "4             5          go   VB\n",
       "5             6          to   IN\n",
       "6             7           a   DT\n",
       "7             8       fancy   JJ\n",
       "8             9  restaurant   NN\n",
       "9            10           .    .\n",
       "10                              \n",
       "11            1        next   JJ\n",
       "12            2    thursday   NN\n",
       "13            3           .    .\n",
       "14                              \n",
       "15            1      dinner   NN\n",
       "16            2           .    .\n",
       "17                              \n",
       "18            1           i  PRP\n",
       "19            2        want  VBP\n",
       "20            3          to   TO\n",
       "21            4         eat   VB\n",
       "22            5      french   JJ\n",
       "23            6        food   NN\n",
       "24            7           .    .\n",
       "25                              \n",
       "26            1          as   RB\n",
       "27            2         far   RB\n",
       "28            3        away   RB\n",
       "29            4          as   IN\n",
       "...         ...         ...  ...\n",
       "133221        2        thai   DT\n",
       "133222        3     cuisine   NN\n",
       "133223        4   yorkshire   NN\n",
       "133224        5        fish   NN\n",
       "133225        6         and   CC\n",
       "133226        7       chips  NNS\n",
       "133227        8     zachary  VBP\n",
       "133228        9          's  POS\n",
       "133229       10     chicago   NN\n",
       "133230       11   yorkshire   NN\n",
       "133231       12        fish   NN\n",
       "133232       13         and   CC\n",
       "133233       14       chips  NNS\n",
       "133234       15           .    .\n",
       "133235                          \n",
       "133236        1         won  VBD\n",
       "133237        2        thai   DT\n",
       "133238        3     cuisine   NN\n",
       "133239        4   yorkshire   NN\n",
       "133240        5        fish   NN\n",
       "133241        6         and   CC\n",
       "133242        7       chips  NNS\n",
       "133243        8     zachary  VBP\n",
       "133244        9          's  POS\n",
       "133245       10     chicago   NN\n",
       "133246       11   yorkshire   NN\n",
       "133247       12        fish   NN\n",
       "133248       13         and   CC\n",
       "133249       14       chips  NNS\n",
       "133250       15           .    .\n",
       "\n",
       "[133251 rows x 3 columns]"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sent = ['the', 'dog', 'ate', 'the', 'food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"'d\", 'like', 'to', 'go', 'to', 'a', 'fancy', 'restaurant', '.']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fixed Vocabulary and Tag Lists\n",
    "return \n",
    "- tag list\n",
    "- vocabuarly list called vocabulary\n",
    "- events list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = ['CC', 'CD',\n",
    "        'DT',\n",
    "        'EX',\n",
    "        'FW',\n",
    "        'IN', \n",
    "        'JJ', 'JJR', 'JJS',\n",
    "        'LS', \n",
    "        'MD',\n",
    "        'NN', 'NNS', 'NNP', 'NNPS',\n",
    "        'PDT', 'POS', 'PRP', 'PRP$',\n",
    "        'RB', 'RBR', 'RBS', 'RP',\n",
    "        'SYM', \n",
    "        'TO', \n",
    "        'UH', \n",
    "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "        'WDT', 'WP', 'WP$', 'WRB', \n",
    "        '$', '#', '\"', '(', ')', ',', '.', ':'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_dict(data, ngrams = \"tag_word\"):\n",
    "    \"\"\"\n",
    "    Creates dict of ngrams (key) and count (value). \n",
    "    \n",
    "    Arguments: \n",
    "        data: DataFrame with 'tag' and 'word' colum\n",
    "        negrams: denote type of ngram (unigram or bigram) and if want words or tags: word_word or tag_word\n",
    "    Returns:\n",
    "        A dict where key is either a unigram or a bigram tuple, and value is the count of the ngrams\n",
    "    \"\"\"\n",
    "    if ngrams == \"tag_tag\":     \n",
    "        col_1 = data['tag']\n",
    "        col_2 = col_1[1:col_1.shape[0]]\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "        ngram_count[('', col_1[0])] += 1\n",
    "\n",
    "    \n",
    "    if ngrams == \"tag_word\": # not really bi-grams, just getting count of tag,word\n",
    "        col_1 = data['word']\n",
    "        col_2 = data['tag']\n",
    "        ngram_count = list(zip(col_1, col_2))\n",
    "        ngram_count = dict(Counter(ngram_count))\n",
    "            \n",
    "    if ngrams == 'tag': \n",
    "        ngram_count = dict(Counter(data.tag))      \n",
    "        \n",
    "    if ngrams == 'word': \n",
    "        ngram_count = dict(Counter(data.word))\n",
    "            \n",
    "    return ngram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fixed vocabulary; identify which words will be considered UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words that we will call unknowns, and replace these instances in the dataframe\n",
    "unigrams = ngram_dict(train, \"word\")\n",
    "unknowns = { key:value for key, value in unigrams.items() if value < 2 }\n",
    "unknowns = unknowns.fromkeys(unknowns, 'UNK')\n",
    "# replace words that appear less than three times with UNK in training data\n",
    "train['word'] = train['word'].replace(unknowns)\n",
    "#get list of vocabulary \n",
    "vocab = ngram_dict(train, \"word\")\n",
    "vocabulary = list(vocab.keys())\n",
    "vocabulary.remove('') # remove spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return index of the words in the new sentece from the fixed vocabuarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "won\n",
      "thai\n",
      "cuisine\n",
      "yorkshire\n",
      "fish\n",
      "and\n",
      "chips\n",
      "zachary\n",
      "'s\n",
      "chicago\n",
      "yorkshire\n",
      "fish\n",
      "and\n",
      "chips\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[443, 451, 82, 385, 762, 1089, 1157, 907, 574, 144, 385, 762, 1089, 1157, 992]"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = []\n",
    "for word in new_sent:\n",
    "    print(word)\n",
    "    try: \n",
    "        events.append(vocabulary.index(word))\n",
    "    except: events.append(vocabulary.index('UNK'))\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Transition and Observation Matrices\n",
    "return\n",
    "-tran matrix\n",
    "-observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_transition_matrix (tags, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the transition matrix (len(tags)+1 x len(tags))\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        bigram_counts: count of bigrams of POS tags in training data (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: 45 x 44 matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    transition = [] # list of transition probabilities \n",
    "    \n",
    "    # first compute the starting probabilities \n",
    "\n",
    "    for x in tags: \n",
    "            pair = ('',x) # here the period denotes the start of a sentence. Not very confident about this\n",
    "            denominator = unigram_counts[''] + len(tags)\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                 numerator = 1\n",
    "            transition.append(numerator / denominator)\n",
    "\n",
    "\n",
    "    # then compute everything else \n",
    "    \n",
    "    for x in tags:\n",
    "        for y in tags:\n",
    "            pair = (x,y)\n",
    "            try:\n",
    "                denominator = unigram_counts[x] + len(tags)\n",
    "            except: \n",
    "                denominator = len(tags)\n",
    "            try: \n",
    "                numerator = bigram_counts[pair] + 1 \n",
    "            except:\n",
    "                numerator = 1 \n",
    "            transition.append(numerator / denominator)\n",
    "   \n",
    "    \n",
    "    transition = np.array(transition)\n",
    "    tran_matrix = transition.reshape(len(tags)+1, len(tags))\n",
    "    \n",
    "    return tran_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00007896,  1.        ,  1.        ,  0.9923554 ,  1.        ,\n",
       "        0.94901961,  1.        ,  0.9950701 ,  1.        ,  1.        ,\n",
       "        0.84615385,  1.        ,  0.97792019,  0.99894715,  0.97271268,\n",
       "        1.        ,  1.        ,  1.        ,  0.99980607,  1.        ,\n",
       "        0.98761545,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.99947871,  1.        ,  0.98099762,\n",
       "        0.99438202,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.00347387,  1.        ])"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tag_counts = ngram_dict(train, \"tag_tag\")\n",
    "unigram_tag_counts = ngram_dict(train, \"tag\")\n",
    "transitions = compute_transition_matrix (tags, bigram_tag_counts, unigram_tag_counts)\n",
    "np.sum(transitions, axis = 1) # confirm that most rows sum closely to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_observation_matrix (tags, vocabulary, bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Compute probabilities for the observation matrix (tags, vocabulary)\n",
    "    \n",
    "    Arguments: \n",
    "        tags: POS tags (that may or may not appear in training data)\n",
    "        vocabulary: words that appear in the training set. Any words that appear less than 2 times = UNK\n",
    "        bigram_counts: count of bigrams of (tag, word) (used for numerator)\n",
    "        unigram_counts: count of unigram POS tag in training data (used for denominator)\n",
    "        \n",
    "    Returns: len(tags) x len(vocabulary) matrix of transition probabilities for all possible POS tags\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    observations = [] # list of observation likelihoods\n",
    "    for x in tags: \n",
    "        for y in vocabulary:\n",
    "            pair = (y, x)\n",
    "            try: \n",
    "                denominator = unigram_counts[x] + len(vocabulary)\n",
    "            except: \n",
    "                 denominator = len(vocabulary)\n",
    "            try: \n",
    "                 numerator = bigram_counts[pair] + 1\n",
    "            except: numerator = 1\n",
    "            observations.append(numerator / denominator)\n",
    "            \n",
    "    observations = np.array(observations)\n",
    "    obs_matrix = observations.reshape(len(tags),len(vocabulary))  \n",
    "    return obs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigram_counts = ngram_dict(train, \"tag_word\")\n",
    "unigram_counts = ngram_dict(train, \"tag\")\n",
    "observations = compute_observation_matrix(tags, vocabulary, bigram_counts, unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(observations, axis = 1) # confirm that most rows sum closely to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm\n",
    "return\n",
    "-predicted POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi (transition, observations, events):\n",
    "    \"\"\" Computes sequnce of hidden states, given observed events.\n",
    "    Arguments: \n",
    "        transition: transition matrix with start probabilites as first row\n",
    "        observations: observation liklihood matrix, with states as rows, and vocabulary as columns\n",
    "        events: sequence of observed events\n",
    "        \n",
    "    Returns: \n",
    "        generator, which yields the states\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition.shape[1]\n",
    "    n_events = len(events)\n",
    "    v = np.zeros((n_states, n_events))\n",
    "    bp = v.copy()\n",
    "    \n",
    "    # initialization step\n",
    "    for s in range(n_states):\n",
    "        v[s,0] = tran[0,s] * observations[s, events[0]]\n",
    "\n",
    "    # induction step\n",
    "    for t in range (1, n_events):\n",
    "        for s in range(n_states):\n",
    "            tmp = []\n",
    "            for s_prime in range (n_states): \n",
    "                prev_t = v[s_prime, t-1]\n",
    "                tran_s_prime_to_s = tran[s_prime + 1, s]\n",
    "                obser_s_given_t = observations[s, events[t]]\n",
    "                tmp.append(prev_t * tran_s_prime_to_s * obser_s_given_t) # still need to changet thos to adding logs\n",
    "            # now that all interim probabilities have been computed for given state, get max\n",
    "            # and also store the index of the argmax\n",
    "            v[s,t] = max(tmp) # log will be negative; so insetad of \n",
    "            bp[s,t] = np.argmax(tmp) # take argmin \n",
    "\n",
    "    # termination step\n",
    "    q = np.argmax(v[:, n_events-1]) # want to get the argmax of the final time -- it will return a state index\n",
    "\n",
    "    # back reference step \n",
    "    for i in reversed(range(n_events)):\n",
    "        yield q\n",
    "        q = int(bp[q,i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(viterbi_gen, names_events):\n",
    "    \"\"\" translate viterbi generater into a sequence of state anme\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for state in viterbi_gen:\n",
    "        name = names_events[state]\n",
    "        sequence.insert(0, name)\n",
    "        \n",
    "    return(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = viterbi(transitions, observations, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amyburkhardt/Documents/NLP/venv/lib/python3.5/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NNPS',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC',\n",
       " 'CC']"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence(tagger, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent(seq, sep):\n",
    "    g = []\n",
    "    for el in seq:\n",
    "        if el == sep:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "    \n",
    "\n",
    "result = list(sent(sentences, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_events(new_sent):\n",
    "    events = []\n",
    "    for word in new_sent:\n",
    "        try: \n",
    "            events.append(vocabulary.index(word))\n",
    "        except: events.append(vocabulary.index('UNK'))\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_pos = []\n",
    "counter = 0\n",
    "for new_sent in result:\n",
    "    if counter > 0: \n",
    "        new_sent.pop(0)\n",
    "    tagger = viterbi(transitions, observations, get_events(new_sent))\n",
    "    print(new_sent)\n",
    "    sequence = get_sequence(tagger, tags)\n",
    "    sequence.insert(len(sequence), '') #add space at the end\n",
    "    all_pos.append(sequence)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in all_pos for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':flat_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':flat_list})\n",
    "train['tag'] = df\n",
    "\n",
    "train.to_csv(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/out_train_922.txt\", sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do list\n",
    "1. make sure I am adding the log of the probabilities, instead of multiplying probabilties in viterbi (and that it still works)\n",
    "2. TONIGHT: write function to print tags from get_sequence into .txt file in a format that can then read in and evaluated by eval.py\n",
    "3. place code in final .py file (make it pretty if have time)\n",
    "4. upon working code, evaluate both training and intermediate test data files\n",
    "5. recompute transition and observation tables with entire training set\n",
    "6. MONDAY NIGHT: wait for final test set to run through system\n",
    "7. MONDAY NIGHT: write up report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_table(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/rand_test.txt\",'\\t', \n",
    "                      header=None, \n",
    "                      skip_blank_lines=False, \n",
    "                      keep_default_na = False,\n",
    "                      names = ['word_Num', 'word','tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = test['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent(seq, sep):\n",
    "    g = []\n",
    "    for el in seq:\n",
    "        if el == sep:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "    \n",
    "\n",
    "result = list(sent(sentences, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_events(new_sent):\n",
    "    events = []\n",
    "    for word in new_sent:\n",
    "        try: \n",
    "            events.append(vocabulary.index(word))\n",
    "        except: events.append(vocabulary.index('UNK'))\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_pos = []\n",
    "counter = 0\n",
    "for new_sent in result:\n",
    "    if counter > 0: \n",
    "        new_sent.pop(0)\n",
    "    tagger = viterbi(transitions, observations, get_events(new_sent))\n",
    "    #print(new_sent)\n",
    "    sequence = get_sequence(tagger, tags)\n",
    "    sequence.insert(len(sequence), '') #add space at the end\n",
    "    all_pos.append(sequence)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in all_pos for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':flat_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':flat_list})\n",
    "test['tag'] = df\n",
    "\n",
    "test.to_csv(\"/Users/amyburkhardt/Dropbox/NLP Readings/hw 1/out_test_922.txt\", sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
